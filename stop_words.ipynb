{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Test\n",
    "Gensim includes a tokenizer, but it mostly handles lowercasing and de-accenting. We're just working with English, so de-accenting isn't a concern.\n",
    "Tokenizing will require a stopwords list, but we don't have a good one for our domain (default lists are generally crap).\n",
    "So, we'll use some scikit learn tools to create a stopword list.\n",
    "But, we want to build the stopword list from the tokenized words, so we'll try out tokenizing, then generating the stopword list, then tokenizing again with the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stoplists = list()\n",
    "\n",
    "def get_stops(documents, max_df=0.75, min_df=1):\n",
    "    \"\"\"\n",
    "    Documents should already have a basic tokenization pass\n",
    "    \"\"\"\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    # first, make the documents consist of regularized text\n",
    "    # lowercase, split contractions, etc.\n",
    "    # this will be deterministic regardless of document set\n",
    "    documents = [\" \".join(tokenizer.tokenize(doc)) for doc in documents]\n",
    "\n",
    "    # use count vectorizer to get stopwords set\n",
    "    # i.e. words appearing in > 70% of documents or less than twice\n",
    "    vectorizer = CountVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=None,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "    )\n",
    "    vectorizer.fit(documents)\n",
    "    return vectorizer.stop_words_\n",
    "\n",
    "\n",
    "def tokenize_docs(documents, tokens_only=False, stops=set()):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        tokens = [token for token in tokenizer.tokenize(doc) if token not in stops]\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_dat_dir = Path(gensim.__path__[0])/\"test\"/\"test_data\"\n",
    "train_corpus = (test_dat_dir/\"lee_background.cor\").read_text(encoding=\"iso-8859-1\").split(\"\\n\")\n",
    "test_corpus = (test_dat_dir/\"lee.cor\").read_text(encoding=\"iso-8859-1\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = get_stops(train_corpus + test_corpus, max_df=0.4)\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(tokenize_docs(train_corpus, stops=stops))\n",
    "test_corpus = list(tokenize_docs(test_corpus, stops=stops, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector([\"only\", \"you\", \"can\", \"prevent\", \"forest\", \"fires\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = list()\n",
    "second_ranks = list()\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, _ in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
